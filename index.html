<!doctype html>
<html>

<head>
  <title>Rethinking Credit Score</title>
  <meta charset="utf-8" name="viewport" content="width=device-width, initial-scale=1">
  <link href="css/frame.css" media="screen" rel="stylesheet" type="text/css" />
  <link href="css/controls.css" media="screen" rel="stylesheet" type="text/css" />
  <link href="css/custom.css" media="screen" rel="stylesheet" type="text/css" />
  <link href='https://fonts.googleapis.com/css?family=Open+Sans:400,700' rel='stylesheet' type='text/css'>
  <link href='https://fonts.googleapis.com/css?family=Open+Sans+Condensed:300,700' rel='stylesheet' type='text/css'>
  <link href="https://fonts.googleapis.com/css?family=Source+Sans+Pro:400,700" rel="stylesheet">
  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script src="js/menu.js"></script>
  <style>
    .menu-index {
      color: rgb(255, 255, 255) !important;
      opacity: 1 !important;
      font-weight: 700 !important;
    }
  </style>
</head>

<body>
  <div class="menu-container"></div>
  <div class="content-container">
    <div class="banner"
      style="background: url('img/all_categories.png') no-repeat center; background-size: cover; height: 200px;">
    </div>
    <div class="banner">
      <div class="banner-table flex-column">
        <div class="flex-row">
          <div class="flex-item flex-column">
            <h2 class="add-top-margin-small">Rethinking Credit Score: Ensuring Fair Lending through NLP for Transaction
              Categorization</h2>
            <p class="text">
              For decades, the banking industry has assessed credit worthiness the same way: they
              use massive amounts of data such as an applicant's current debt, the accounts
              they have opened, and the ratio of money owed to available credit to
              determine loan qualification. This system has been generally successful
              in the past, however it is not fair for those with no/low credit history,
              such as immigrants or young adults who are trying to build. Thus, it can be extremely
              difficult for these applicants, often referred to as being “credit
              invisible”, to be approved for loans or other forms of credit.
              Hence, in this paper, we introduced a more comprehensive assessment
              framework that allows individuals to submit their past banking
              history as a supplementary material to better assess credit worthiness.
            </p>
          </div>
        </div>
      </div>
    </div>
    <div class="content">
      <div class="content-table flex-column">
        <!-------------------------------------------------------------------------------------------->
        <!--Start Intro-->
        <div class="flex-row">
          <div class="flex-item flex-column">
            <h2>Introduction</h2>
            <hr>
            <img class="image" src="img/traditional.png">
          </div>
        </div>
        <div class="flex-row">
          <div class="flex-item flex-column">
            <p class="text add-top-margin">
              During a loan/credit approval. There are several processes applicants
              needs to go through:
            <ul>
              <li><b>Application</b>: Applicants fill out a loan application form for a credit card or other personal
                loan.
              </li>
              <li><b>Submission</b>: Applicants submit the form along with required documents* to the lenders for
                review.</li>
              <li><b>Review</b>: Lenders evaluate applicants’ credit worthiness (credit scores) using credit scoring
                models.</li>
              <li><b>Approval</b>: High credit scores gained pre-approval, but banks checks documents for final
                approval.</li>
            </ul>
            </p>
          </div>
        </div>
        <!--End Intro-->
        <!-------------------------------------------------------------------------------------------->
        <!--Start Text Only-->
        <div class="flex-row">
          <div class="flex-item flex-column">
            <h2>Draw Backs</h2>
            <hr>
            <p class="text">
              When an applicant applies for a loan or a credit card, lenders such as banks look at the applicant's bank
              statements
              to determine their eligibility. This may includes checking for:
            <ul>
              <li>Savings and cash flow</li>
              <li>Unusual deposits</li>
              <li>Debt</li>
            </ul>
            As well as red flags such as:
            <ul>
              <li>Non-sufficient funds fees</li>
              <li>Large, undocumented deposits</li>
            </ul>
            Overall, a lender or credit card companies would like to know more about an applicant through
            his/her bank statements. They would like to verify savings, unusual activities, and make sure
            the applicants' ability to pay back debts.

            Even though this verification is sufficient enough to understand the general public, it fails to
            consider applicants who are underrepresented. For example, immigrants and young adults
            typically do not have a stable funds in the USA but often bring money in from their home country. The
            activities of these large overseas deposits often hinders their available credit or their ability for credit
            approval, when in reality they should be eligible.
            </p>

            <p>
              Therefore, we hope to answer the question:
            </p>
            <p class="text-center text-large text-italic" style="color: #ff0000;">
              How can we utilize NLP to enable the "credit invisible,” such as
              young adults and immigrants without an established line of credit,
              to have an equal opportunity for fair lending?
            </p>
          </div>
        </div>
        <!--End Text Only-->
        <!-------------------------------------------------------------------------------------------->
        <!-- Start Methods -->
        <div class="flex-ro">
          <div class="flex-item flex-column">
            <h2>Methods</h2>
            <hr>
            <p class="text">
              Recall the factors lenders hope to (or not to) see in a banking statement,
              we see that these factors may not generalize to people of all demographics.
              Therefore we attempted to extract more information from banking
              statement by: determining the category of each transaction using its
              date, amount, and most importantly, statement memo.
            </p>
            <img class="image" src="img/demo.png">
            <p class="image-caption">Using Data to Predict Categories</p>
            <p>To get to this stage, we first worked to transform or engineer our features into
              suitable format to be able to build our models.
            </p>
            <h3>Natural language processing</h3>
            <hr>
            <p>
              <a href="https://www.ibm.com/topics/natural-language-processing">Natural language processing</a>
              is an ability for a computer to process
              and interpret human language. Its application can range from language
              translation to text summarization. In this case, we are using NLP to transform our memo column in order to
              build a model summarizing a user's transactions (on bank statement)
              into 8 different categories.
            <ul>
              <li>Food and Beverages</li>
              <li>Entertainment</li>
              <li>General Merchandise</li>
              <li>Travel</li>
              <li>Automotive</li>
              <li>Healthcare/Medical</li>
              <li>Groceries</li>
              <li>Pets/Pet Care</li>
            </ul>
            To categorize, or summarize the memo, we must find a way to transform the text into something our model
            could understand. There are many different NLP methods that range in complexity and computing power to do
            such tasks. In our use case, the memo field has very little structure/context for the algorithm
            to learn. Therefore, we attempted to use TF-IDF to transform the text into feature vectors
            since it focuses on characters and unigrams, bigrams, and trigrams rather than the full context of a
            sentence.
            </p>
            <h4>TF-IDF</h4>
            <hr>
            <p><a href="https://www.capitalone.com/tech/machine-learning/understanding-tf-idf/"
                target="_blank">TF-IDF</a>, term frequency-inverse document frequency, quantify the relevance
              of a word in a given document among the collection of documents. It
              counts the term frequency relative to documents. Therefore, at the end we
              would obtained a TF-IDF features that have counts according to each transaction.
              Here we then obtain:
            <ul>
              <li>Frequency a word appear per transaction</li>
              <li>Frequency a word appear in corpus of transaction</li>
            </ul>
            </p>
            <h4>Text Cleaning</h4>
            While TF-IDF is keeping track of the term frequency, it would be best
            to get rid off some unnecessary words and fix some of the vocab that may
            have the same meaning so that TF-IDF matrix is not unnecessary large.
            For example:
            <ul>
              <li>Stop words: a, an ,the</li>
              <li>Puncuation: <strong>, : . </strong></li>
              <li>Capitalization: WALMART -> walmart</li>
            </ul>
            <h3>Non-text Feature Engineering</h3>
            <hr>
            <p>To maximize information for future task, we feature engineered the
              non-text data (<i>date, amount</i>) includes:
            <ul>
              <li>Standardization on <i>amount</i></li>
              <li>Whether the amount is whole number</li>
              <li>Create more features from <i>date</i></li>
              <ul>
                <li>Year</li>
                <li>Month</li>
                <li>Day</li>
                <li>Weekend</li>
                <li>Holiday</li>
              </ul>
            </ul>
            The reason we created this features is because category sizes may vary on certain days of the week/month
            /year. People might spend more on dining during the weekends, spend more on general
            merchandise in december for Christmas. It could be a feature to indicate the behavior of how the categories
            are distributed.
            </p>
            <h3>Models</h3>
            <hr>
            <p> Once we have prepared our data into a suitable format. We already to proceed to train our model. Since
              we have to types of data: text-only and non-text. We decided to create an ensemble model of two sub
              models: <strong>XGBoost</strong> and <strong>Logistic Regression</strong>. XGBoost model will be trained
              with the non-text features while the logistic regression will be supplied with the text-only features.
            </p>
            <h4>XGBoost</h4>
            <p class="text">
              <img class="image image-wrap-text max-width-400" src="img/Boosting.png">
              <a href="https://xgboost.readthedocs.io/en/stable/tutorials/model.html" target="_blank">XGBoost</a>,
              or extreme gradient boosting, it is a technique modified to build a strong classifer from a number of
              weak
              classifiers. Traditionally, the model is built from a series of smaller models. At first, the training
              data will be train on the first model, then the mislabeled instances will pass down to the second
              model until all the training point is correctly classified or the maximum number of the smalled model is
              reached. As a result, the final model becomes a linear combination of smaller models. During the
              process, the weights of the training data are tweaked for the next model. In XGboost, the weights are
              adjusted with the residual errors of the predecessors. This type of technique can be used to predict
              regression, classification, ranks, and even user-defined predictions.
            </p>
            <h4>Logistic Regression</h4>
            <p class="text">
              <img class="image image-wrap-text max-width-400" src="img/Logit.png">
              <a href="https://www.ibm.com/topics/logistic-regression" target="_blank">Logistic Regression</a>, is often
              used in classification problems. It was derived from an old technique, linear regression, that estimates
              the probability of an event happened given the dataset. It is often important to utilize the model's
              probability output rather than it's direct classification output, especially in our case where we are
              looking to
              combine different models.
            </p>
            <!-------------------------------------------------------------------------------------------->
          </div>
        </div>
        <div class=" content">
          <div class="content-table flex-column">
            <!-------------------------------------------------------------------------------------------->
            <!--Start Text with Confusion Matrix-->
            <div class="flex-row">
              <div class="flex-item flex-column">
                <h2 class="no-top-margin">Results</h2>
                <hr>
                <h3>Scoring Metric: Accuracy</h3>
                <hr>
                <p class="text">
                  There are many evaluation metrics we could use for this classification problem. We chose accuracy
                  because of
                  the nature of the problem. We introduce definitions of each metric we considered and our reasoning for
                  prioritizing accuracy:
                <ul>
                  <li><strong>Accuracy</strong> : measures the number of correctly classified labels over all labels.
                  </li>
                  <li><strong>Precision</strong>: measures the number of correctly classified lables over all labels,
                    assuming all the prediction falls into 1 class.</li>
                  <li><strong>Recall</strong>: measures the number of correctly classified lables over all labels,
                    assuming all the true labels falls into 1 class.</li>
                  <li><strong>F1 Score</strong>: measures the harmonic mean of precision and recall.</li>
                </ul>
                The main reason for checking F1 score is ensuring the reliability of the model on both the opposing
                classes. Often in looking solely at accuracy, the mislabeled minority observations are overlooked which
                is a problem in some use cases. For our case, the consequence for misclassification in the minority
                class is not
                very high. For example, it
                doesn't really matter if we incorrectly identifying some of the automotive class.
                In a case like detecting cancer, though, we might want to utilize recall because we don't want any false
                negatives. A false negative meaning a person had cancer and it was not detected. In our case, the cost
                of misclassifying any particular class is not particularly significant as explained in the automotive
                example. Accuracy simply takes the
                overall number of correct classifications, and divides it by the total number of classifications made.
                It doesn't focus on any particular class, because all classes are of equal importance.
                </p>
                <p class="text text-center graph-title">
                  Confusion Matrix for 8 Categories on Ensemble Model
                </p>
                <img class="image center max-width-500 add-top-margin-small" src="img/confusion.jpeg">
                <p class="text">
                  Above is the confusion matrix of all classes. Through the diagonal matrix, we see the all the
                  probability of predicted label being correctly labeled.
                <div class="custom-table-container center add-top-margin-small">
                  <table class="custom-table">
                    <thead>
                      <tr class="bg-color-gray">
                        <th>Category</th>
                        <th class="text-center">Accuracy</th>
                      </tr>
                    </thead>
                    <tbody>
                      <tr>
                        <td>Automotive</td>
                        <td class="text-center">90%</td>
                      </tr>
                      <tr class="bg-color-light-gray">
                        <td>Entertainment</td>
                        <td class="text-center">91%</td>
                      </tr>
                      <tr>
                        <td>Food and Beverages</td>
                        <td class="text-center">87%</td>
                      </tr>
                      <tr class="bg-color-light-gray">
                        <td>General Merchandise</td>
                        <td class="text-center">86%</td>
                      </tr>
                      <tr>
                        <td>Groceries</td>
                        <td class="text-center">85%</td>
                      </tr>
                      <tr class="bg-color-light-gray">
                        <td>Healthcare/Medical</td>
                        <td class="text-center">80%</td>
                      </tr>
                      <tr>
                        <td>Pets/Pet care</td>
                        <td class="text-center">97%</td>
                      </tr>
                      <tr class="bg-color-light-gray">
                        <td>Travel</td>
                        <td class="text-center">93%</td>
                      </tr>
                    </tbody>
                    <tfoot>
                      <tr>
                        <td>Average</td>
                        <td class="text-center">88.6%</td>
                      </tr>
                    </tfoot>
                  </table>
                </div>
                </p>
              </div>
            </div>
          </div>
          <!-------------------------------------------------------------------------------------------->
        </div>
        <div class="flex-row">
          <div class="flex-item flex-column">
            <h2>Conclusion</h2>
            <hr>
            <p class="text">
              Despite credit score being a significant metric for lenders during the loan application approval process,
              this single metric fails to consider how underrepresented demographics may be deemed as unworthy of credit
              when in fact they should be worthy. This is a lose-lose for both lenders and applicants. On one hand,
              lenders lose
              potential customers ("the credit invisible") who are excluded due to this traditional process. At the same
              time, these applicants are not allowed to receive credit, which makes things like buying a house seemingly
              impossible. We aim to explore the limitations of traditional credit score models and propose an
              alternative method for determining creditworthiness that is more inclusive and equitable. We approach this
              problem by utilizing supplmental feautures like a user's categorized transaction history to the
              traditional credit scoring model. With a user's bank statements, we could extract
              information like the transaction date, amount, and memo to flag each transaction into a category (with 89%
              accuracy). The next
              step would be to utilize these categories in creating new features to optimize common credit scoring
              models to make them stronger and more fair. This approach solves both of the problems we described above
              as it
              will aid applicants with low/no credits as well as profit-hungry institutions looking to acquire more
              customers in financial industry.
            </p>
          </div>
        </div>


        <div class="content">
          <div class="content-table flex-column">
            <!-------------------------------------------------------------------------------------------->

            <!--Authors-->
            <div class="flex-item flex-column">
              <p class="text text-large">
                <a target="_blank" href="https://www.linkedin.com/in/chungenpan/">Chung En Pan</a>, cepan@ucsd.edu<br>
                <a target="_blank" href="javascript:void(0)">Kyle Nero</a>, knero@ucsd.edu<br>
                <a target="_blank" href="https://www.linkedin.com/in/koosha-jadbabaei/">Koosha Jadbabaei</a>,
                kjadbaba@ucsd.edu<br>
                <a target="_blank" href="https://www.linkedin.com/in/nathan-van-lingen-a78a2a160/">Nathan Van
                  Lingen</a>, nvanling@ucsd.edu<br>
              </p>
            </div>
            <!--Reference-->
            <div class="flex-item flex-column">
              <p class="text text-italic text-small">
                <a target="_blank"
                  href="https://themortgagereports.com/22079/bank-statements-3-things-mortgage-lenders-dont-want-to-see">Bank
                  statements: 3 things mortgage lenders don't want to see</a><br>
                <a target="_blank"
                  href="https://towardsdatascience.com/accuracy-is-not-enough-for-classification-task-47fca7d6a8ec">Accuracy
                  is NOT enough for Classification Tasks</a><br>
                <a target="_blank" href="https://www.geeksforgeeks.org/xgboost/">XGBoost</a><br>
                <a target="_blank"
                  href="https://www.capitalone.com/tech/machine-learning/understanding-tf-idf/">Understanding TF-IDF for
                  Machine Learning</a><br>
                <a target="_blank" href="https://www.ibm.com/topics/natural-language-processing">What is natural
                  language processing?</a><br>
                <a target="_blank" href="https://xgboost.readthedocs.io/en/stable/tutorials/model.html">Introduction to
                  Boosted Trees</a><br>
                <a target="_blank " href="https://www.ibm.com/topics/logistic-regression">What is logistic
                  regression?</a><br>
                <a target="_blank " href="https://www.saedsayad.com/logistic_regression.htm">Logistic Regression</a><br>


              </p>
            </div>
            <!-------------------------------------------------------------------------------------------->
          </div>
        </div>
        <div class="banner">
          <div class="banner-table flex-column">
            <div class="flex-row">
              <div class="flex-item flex-column">
                <p class="text add-bottom-margin-large">
                </p>
              </div>
            </div>
          </div>
        </div>
      </div>
</body>

</html>