<!doctype html>
<html>

<head>
  <title>Rethink Credit score</title>
  <meta charset="utf-8" name="viewport" content="width=device-width, initial-scale=1">
  <link href="css/frame.css" media="screen" rel="stylesheet" type="text/css" />
  <link href="css/controls.css" media="screen" rel="stylesheet" type="text/css" />
  <link href="css/custom.css" media="screen" rel="stylesheet" type="text/css" />
  <link href='https://fonts.googleapis.com/css?family=Open+Sans:400,700' rel='stylesheet' type='text/css'>
  <link href='https://fonts.googleapis.com/css?family=Open+Sans+Condensed:300,700' rel='stylesheet' type='text/css'>
  <link href="https://fonts.googleapis.com/css?family=Source+Sans+Pro:400,700" rel="stylesheet">
  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.3.1/jquery.min.js"></script>
  <script src="js/menu.js"></script>
  <style>
    .menu-index {
      color: rgb(255, 255, 255) !important;
      opacity: 1 !important;
      font-weight: 700 !important;
    }
  </style>
</head>

<body>
  <div class="menu-container"></div>
  <div class="content-container">
    <div class="banner"
      style="background: url('img/all_categories.png') no-repeat center; background-size: cover; height: 200px;">
    </div>
    <div class="banner">
      <div class="banner-table flex-column">
        <div class="flex-row">
          <div class="flex-item flex-column">
            <h2 class="add-top-margin-small">Rethink Credit Score: Ensuring Fair Lending through NLP for Transaction
              Categorization</h2>
            <p class="text">
              For decades, the banking industry has assessed credit worthiness the same way: they
              use massive amounts of data such as an applicant's current debt, the accounts
              they have opened, and the ratio of money owed to available credit to
              determine loan qualification. This system has been generally successful
              in the past, however it is not fair for those with no credit history,
              such as immigrants or young adults. Thus, it can be extremely
              difficult for these applicants, often referred to as being “credit
              invisible”, to be approved for loans or other forms of credit.
              Hence, in this paper, we introduced a more comprehensive assessment
              framework that allows individuals to submit their past banking
              history as a supplementary material to better assess credit worthiness.
            </p>
          </div>
        </div>
      </div>
    </div>
    <div class="content">
      <div class="content-table flex-column">
        <!-------------------------------------------------------------------------------------------->
        <!--Start Intro-->
        <div class="flex-row">
          <div class="flex-item flex-column">
            <h2>Introduction</h2>
            <hr>
            <img class="image" src="img/traditional.png">
          </div>
        </div>
        <div class="flex-row">
          <div class="flex-item flex-column">
            <p class="text add-top-margin">
              During a loan/credit approval. There are several processes applicants
              needs to go through:
            <ul>
              <li><b>Application</b>: Applicants fill out an application form and for
                loan/credit cards. </li>
              <li><b>Submission</b>: Applicants submit the form along with required
                documents to the bank for review. Required documents may include
                Personal financial statement, itentify verification, etc.</li>
              <li><b>Review</b>: Banks evaluate applicants' eligibility (credit scores)
                using credit scoring models with the information provided.</li>
              <li><b>Approval</b>: High credit score gained conditionally approved,
                banks checks documents for final approval. During Loan processes,
                banks communicate the terms and conditions with the applicants.
                If applicants accepts the terms, the loan is then approved.</li>
            </ul>
            </p>
          </div>
        </div>
        <!--End Intro-->
        <!-------------------------------------------------------------------------------------------->
        <!--Start Text Only-->
        <div class="flex-row">
          <div class="flex-item flex-column">
            <h2>Draw Backs</h2>
            <hr>
            <p class="text">
              When an applicant apply for a loan or a credit card, Lenders such as banks look at your bank statements
              to determine the eligibility of the approval This may include:
            <ul>
              <li>Enough cash for the down payments</li>
              <li>Monthly payment currently holding</li>
              <li>Enough Cash flow for the monthly payments</li>
              <li>Emergency funds</li>
            </ul>
            As well as red flags such as:
            <ul>
              <li>Non-sufficient funds fees</li>
              <li>Large, undocumented deposits</li>
            </ul>
            Overall, a lender or credit card companies would like to know more about an applicant through
            his/her bank statements. They would like to verify savings, unusual activities, and make sure
            the applicants' ability to pay back debts.

            Even though this verification is sufficient enough to understand the general public, it fails to
            consider applicants who are underrepresented. For example, immigrants and young adults
            typically do not have a stable funds in the states but are sourced from parents in foreign
            country. The activities of the large deposits hinder their available credit or even the
            eligibility of acquiring a loan, when in fact, they have sufficient and stable funds to
            suffiecient.
            </p>

            <p>
              Therefore, we hope to answer the question:
            </p>
            <p class="text-center text-large text-italic" style="color: #ff0000;">
              How can we utilize NLP to enable the "credit invisible,” such as
              young adults and immigrants without an established line of credit,
              to have an equal opportunity for fair lending?
            </p>
          </div>
        </div>
        <!--End Text Only-->
        <!-------------------------------------------------------------------------------------------->
        <!-- Start Methods -->
        <div class="flex-ro">
          <div class="flex-item flex-column">
            <h2>Methods</h2>
            <hr>
            <p class="text">
              Recalled the factors lenders hope to (or not) see in the banking statement,
              we see that these factors may not generalize to all demographic.
              Therefore we attempted to extract more information from banking
              statement by: determining the category of each transaction using its
              date, amount, and most importantly, memo.
            </p>
            <img class="image" src="img/demo.png">
            <p class="image-caption">Using Data to Predict Categories</p>
            <p>However, to get to this stage, we have to transform our feature into
              suitable format for it to work.
            </p>
            <h3>Natural language processing</h3>
            <hr>
            <p>
              <a href="https://www.ibm.com/topics/natural-language-processing">Natural language processing</a>
              is an ability for a computer to processes
              and interpret human language. Its application can range from language
              translation to text summarization. In this case, we are summarizing
              the memo into 8 different categories.
            <ul>
              <li>Food and Beverages</li>
              <li>Entertainment</li>
              <li>General Merchandise</li>
              <li>Travel</li>
              <li>Automotive</li>
              <li>Healthcare/Medical</li>
              <li>Groceries</li>
              <li>Pets/Pet Care</li>
            </ul>
            To categorize, or summarize the memo. we must understand the information
            that was given in the memo. In a typical setting, for an algorithm
            to learn a piece of text, it looks at a subset of words in front/back
            of a vocabulary and rank them quantitatively according to their relevance.
            Hence the structure of the text is crucial to the algorithm. However, in
            the memo field, there is little to no structure/context for the algorithm
            to learn. Therefore, we attempted to use Tf-idf to extract information
            since it does not take into account the semantic/context.
            </p>
            <h4>TF-IDF</h4>
            <hr>
            <p><a href="https://www.capitalone.com/tech/machine-learning/understanding-tf-idf/"
                target="_blank">TF-IDF</a>, term frequency-inverse document frequency, quantify the relevance
              of a word in a given document among the collection of documents. It
              counts the term frequency relative to documents. Therefore, at the end we
              would obtained a TF-IDF features that have counts according to each transaction.
              Here we then obtain:
            <ul>
              <li>Frequency a word appear per transaction</li>
              <li>Frequency a word appear in corpus of transaction</li>
            </ul>
            </p>
            <h4>Text Cleaning</h4>
            While TF-IDF is keeping track of the term frequency, it would be best
            to get rid off some unnecessary words and fix some of the vocab that may
            have the same meaning so that TF-IDF matrix is not unnecessary large.
            For example:
            <ul>
              <li>Stop words: a, an ,the</li>
              <li>Puncuation: <strong>, : . </strong></li>
              <li>Capitalization: WALMART -> walmart</li>
            </ul>
            <h3>Non-text Feature Engineering</h3>
            <hr>
            <p>To maximize information for future task, we feature engineered the
              non-text data (<i>date, amount</i>) includes:
            <ul>
              <li>Standardization on <i>amount</i></li>
              <li>Whether the amount is whole number</li>
              <li>Create more features from <i>date</i></li>
              <ul>
                <li>Year</li>
                <li>Month</li>
                <li>Day</li>
                <li>Weekend</li>
                <li>Holiday</li>
              </ul>
            </ul>
            The reason we created this features is because category sizes may vary on certain days of the week/month
            /year. People might spend more on dining during the weekends, spend more on general
            merchandise in december for christmas. It could be a feature to indicate the behavior of how the categories
            are distributed.
            </p>
            <h3>Models</h3>
            <hr>
            <p> Once we have prepared our data into a suitable format. We already to proceed to train our model. Since
              we have to types of data: text-only and non-text. We decided to create an ensemble model of two sub
              models: <strong>XGBoost</strong> and <strong>Logistic Regression</strong>. XGBoost model will be trained
              with the non-text features while the logistic regression will be supplied with the text-only features.
            </p>
            <h4>XGBoost</h4>
            <p class="text">
              <img class="image image-wrap-text max-width-400" src="img/Boosting.png">
              <a href="https://xgboost.readthedocs.io/en/stable/tutorials/model.html" target="_blank">XGBoost</a>,
              or extreme gradient boosting, it is a technique modified to build a strong classifer from a number of of
              weak
              classifier. Traditionally, the model is built from a series of smaller models. At first, the training
              data will be train on the first model, then its mislabeled instances will then passes down to the second
              model until all the training point is correctly classified or the maximum number of the smalled model is
              reached. This method result the final model being a linear combination of smaller models. During the
              process, the weights of the training data is tweaked for the next model, but in XGboost, the weights is
              adjusted with the residual errors of the predecessors. This type of technique can be used to predict
              regression, classification, ranks, and even user-defined predictions.
            </p>
            <h4>Logistic Regression</h4>
            <p class="text">
              <img class="image image-wrap-text max-width-400" src="img/Logit.png">
              <a href="https://www.ibm.com/topics/logistic-regression" target="_blank">Logistic Regression</a>, is often
              used in classification problem. It was derived from an old technique, linear regression, that estimates
              the probability of an event happened given the dataset. Some may asked rather than probability, why don't
              we just predict an event happened or not. The reason we would want a probability is that dataset may not
              have enough information to perfectly predict the event happen. Also we may not have a classifier that
              perfectly separate the class hence we have probability to account for the uncertainty of an event.
            </p>
            <!-------------------------------------------------------------------------------------------->
          </div>
        </div>
        <div class=" content">
          <div class="content-table flex-column">
            <!-------------------------------------------------------------------------------------------->
            <!--Start Text with Confusion Matrix-->
            <div class="flex-row">
              <div class="flex-item flex-column">
                <h2 class="no-top-margin">Results</h2>
                <hr>
                <h3>Scoring Metric: Accuracy</h3>
                <hr>
                <p class="text">
                  There are many evaluation metrics for classification problem, and here we chose acuuracy because of
                  the nature of the problem. We will introduce some definition of each metrics and explain why we
                  chose accuracy.
                <ul>
                  <li><strong>Accuracy</strong> : measures the number of correctly classified labels over all labels.
                  </li>
                  <li><strong>Precision</strong>: measures the number of correctly classified lables over all labels,
                    assuming all the prediction falls into 1 class.</li>
                  <li><strong>Recall</strong>: measures the number of correctly classified lables over all labels,
                    assuming all the true labels falls into 1 class.</li>
                  <li><strong>F1 Score</strong>: measures the harmonic mean of precision and recall.</li>
                </ul>
                </p>
                <p class="text text-center graph-title">
                  Confusion Matrix for 8 Categories on Ensemble Model
                </p>
                <img class="image center max-width-400 add-top-margin-small" src="">
                <p class="text">

                </p>
              </div>
            </div>
          </div>
          <!-------------------------------------------------------------------------------------------->
        </div>
        <div class="content">
          <div class="content-table flex-column">
            <!-------------------------------------------------------------------------------------------->

            <!--Authors-->
            <div class="flex-item flex-column">
              <p class="text text-large">
                <a target="_blank" href="javascript:void(0)">Chung En Pan</a>, cepan@ucsd.edu<br>
                <a target="_blank" href="javascript:void(0)">Kyle Nero</a>, knero@ucsd.edu<br>
                <a target="_blank" href="javascript:void(0)">Koosha Jadbabaei</a>, kjadbaba@ucsd.edu<br>
                <a target="_blank" href="javascript:void(0)">Nathan Van Lingen</a>, nvanling@ucsd.edu<br>
              </p>
            </div>
            <!--Reference-->
            <div class="flex-item flex-column">
              <p class="text text-small">
                <a target="_blank"
                  href="https://themortgagereports.com/22079/bank-statements-3-things-mortgage-lenders-dont-want-to-see">Bank
                  statements: 3 things mortgage lenders don't want to see</a>
              </p>
            </div>
            <!-------------------------------------------------------------------------------------------->
            <!--Start Credits-->
            <div class="flex-row">
              <div class="flex-item flex-item-stretch flex-column">
                <p class="text text-small text-italic add-top-margin-large">
                  Credits: <span class="highlight-text">Organization One</span>: Author One and Author Two / <span
                    class="highlight-text">Organization Two</span>: Author Three and Author Four
                </p>
              </div>
            </div>
            <!--End Credits-->
            <!-------------------------------------------------------------------------------------------->
          </div>
        </div>
        <div class="banner">
          <div class="banner-table flex-column">
            <div class="flex-row">
              <div class="flex-item flex-column">
                <h2>This is the footer area</h2>
                <p class="text add-bottom-margin-large">
                  Phasellus viverra nulla ut metus varius laoreet. Quisque rutrum. Aenean imperdiet. Etiam ultricies
                  nisi
                  vel
                  augue. Curabitur ullamcorper ultricies nisi. Nam eget dui. Etiam rhoncus. Maecenas tempus, tellus
                  eget
                  condimentum rhoncus, sem quam semper libero, sit amet adipiscing sem neque sed ipsum. Nam quam nunc,
                  blandit
                  vel, luctus pulvinar, hendrerit id, lorem.
                </p>
              </div>
            </div>
          </div>
        </div>
      </div>
</body>

</html>